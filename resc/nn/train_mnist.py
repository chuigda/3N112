#!/usr/bin/env python3

# This file is generated by Gemini-2.5 Pro, and proven to be working.
# Dependency setup:
#
#    pip install torch torchvision numpy
#
# If you don't have appropriate Python environment, consider using the download_pretrained.sh
# to download the pre-trained weights and biases.

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import os

# 1. 定义网络结构 (784 -> 300 -> 100 -> 10)
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.layers = nn.Sequential(
            # 第1层: 784 -> 300
            nn.Linear(784, 300),
            nn.Sigmoid(),
            # 第2层: 300 -> 100
            nn.Linear(300, 100),
            nn.Sigmoid(),
            # 第3层: 100 -> 10
            nn.Linear(100, 10)
            # 注意：最后一层不加 Sigmoid 或 Softmax，
            # 因为 nn.CrossEntropyLoss 会为我们处理。
        )

    def forward(self, x):
        # 将输入的 28x28 图像展平为 784 的向量
        x = x.view(-1, 784)
        return self.layers(x)

# 2. 准备数据
print("正在下载和准备 MNIST 数据集...")
# 数据预处理：转换为 Tensor，像素值会从 [0, 255] 缩放到 [0.0, 1.0]
transform = transforms.ToTensor()

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)
print("数据准备完成。")

# 3. 训练模型
print("\n开始训练模型...")
model = MLP()
criterion = nn.CrossEntropyLoss() # 这是一个适合多分类任务的损失函数
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 5 # 训练 5 个周期对于 MNIST 已经能达到不错的精度

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # 前向传播
        outputs = model(images)
        loss = criterion(outputs, labels)

        # 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (i+1) % 300 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')

print("模型训练完成。")

# 4. 测试模型精度
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    print(f'\n模型在测试集上的精度: {100 * correct / total:.2f} %')

# 5. 导出权重和偏置
print("\n正在导出权重和偏置到文件...")

# 创建一个目录来存放权重文件
output_dir = "mnist_weights"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# 提取每一层的参数
# model.layers[0] 是第一个 nn.Linear(784, 300)
# model.layers[2] 是第二个 nn.Linear(300, 100)
# model.layers[4] 是第三个 nn.Linear(100, 10)
layer_indices = [0, 2, 4]
layer_names = ["L1_784x300", "L2_300x100", "L3_100x10"]

for i, name in zip(layer_indices, layer_names):
    layer = model.layers[i]

    # 提取权重和偏置，并转换为 numpy 数组
    weights = layer.weight.data.cpu().numpy()
    biases = layer.bias.data.cpu().numpy()

    # 展平权重矩阵，以匹配 GLSL 中的一维缓冲区
    weights_flat = weights.flatten()

    # 定义文件名
    weights_filename = os.path.join(output_dir, f"weights_{name}.bin")
    biases_filename = os.path.join(output_dir, f"biases_{name}.bin")

    # 以二进制格式保存
    weights_flat.astype(np.float32).tofile(weights_filename)
    biases.astype(np.float32).tofile(biases_filename)

    print(f"已保存: {weights_filename} (大小: {weights_flat.size} 个浮点数)")
    print(f"已保存: {biases_filename} (大小: {biases.size} 个浮点数)")

print(f"\n所有权重和偏置已成功导出到 '{output_dir}' 目录下！")
